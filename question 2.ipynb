{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#Use pd_speech_features.csv perform scaling\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('pd_speech_features.csv')\n\n# Split the data into features and target variable\nX = df.iloc[:, 1:-1]\ny = df.iloc[:, -1]\n\n# Standardize the features\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# Create a new dataframe with the standardized features\nstd_df = pd.DataFrame(X_std, columns=X.columns)\n\n# Add the target variable to the new dataframe\nstd_df['Target'] = y\n\n# Visualize the standardized data\nprint(std_df.head())\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": "     gender       PPE       DFA      RPDE  numPulses  numPeriodsPulses  \\\n0  0.968742  0.627644  0.256144  0.605835  -0.846892         -0.842373   \n1  0.968742  0.121620 -0.080433  0.368415  -0.907404         -0.902773   \n2  0.968742  0.617950 -0.349839  0.733609  -0.927575         -0.922907   \n3 -1.032266 -1.980560  1.382279  0.753631  -1.472186         -1.466513   \n4 -1.032266 -2.472989  1.398068  0.300123  -0.887233         -0.882640   \n\n   meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  ...  \\\n0          0.933328           -0.407251     -0.054993      0.037843  ...   \n1          1.040014           -0.426092     -0.142570     -0.027698  ...   \n2          1.084576           -0.443557     -0.214916     -0.088871  ...   \n3          2.464215           -0.275316      0.710353      1.256919  ...   \n4          0.987044            3.143597      1.152045      1.178269  ...   \n\n   tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n0                  -0.584822                  -0.619412   \n1                  -0.584895                  -0.589778   \n2                  -0.584767                  -0.629033   \n3                  -0.532242                  -0.591137   \n4                  -0.475545                  -0.521356   \n\n   tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n0                  -0.576762                  -0.482286   \n1                   0.193084                   0.016183   \n2                  -0.356261                  -0.156055   \n3                  -0.522406                   0.008400   \n4                  -0.490090                  -0.404833   \n\n   tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n0                  -0.399331                  -0.484533   \n1                  -0.067120                  -0.175566   \n2                  -0.067593                  -0.463462   \n3                  -0.449894                  -0.470865   \n4                  -0.249678                  -0.042021   \n\n   tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n0                  -0.775137                  -0.814727   \n1                  -0.526647                  -0.582972   \n2                  -0.756063                  -0.804390   \n3                  -0.633475                  -0.588387   \n4                  -0.419354                  -0.672216   \n\n   tqwt_kurtosisValue_dec_36  Target  \n0                  -0.366595       1  \n1                   0.400396       1  \n2                  -0.780935       1  \n3                  -0.801583       1  \n4                  -0.741477       1  \n\n[5 rows x 754 columns]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Use pd_speech_features.csv to Apply PCA (k=3)\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('pd_speech_features.csv')\n\n# Split the data into features and target variable\nX = df.iloc[:, 1:-1]\ny = df.iloc[:, -1]\n\n# Standardize the features\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# Create a PCA object with 3 principal components\npca = PCA(n_components=3)\n\n# Fit the PCA model on the standardized data\npca.fit(X_std)\n\n# Transform the data to the new coordinate system\nX_pca = pca.transform(X_std)\n\n# Visualize the explained variance ratio of the principal components\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n\n# Create a new dataframe with the transformed data\npca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2', 'PC3'])\n\n# Add the target variable to the new dataframe\npca_df['Target'] = y\n\n# Visualize the transformed data\nprint(pca_df.head())\n\n\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "text": "Explained Variance Ratio: [0.12961998 0.09390046 0.08252524]\n         PC1       PC2        PC3  Target\n0 -10.034309  1.473186  -6.836297       1\n1 -10.624667  1.585846  -6.820882       1\n2 -13.503155 -1.251541  -6.809193       1\n3  -9.143503  8.834663  15.302885       1\n4  -6.752753  4.612583  15.649159       1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Use pd_speech_features.csv to Use SVM to report performance \n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Load the dataset\ndf = pd.read_csv('pd_speech_features.csv')\n\n# Split the data into features and target variable\nX = df.iloc[:, 1:-1]\ny = df.iloc[:, -1]\n\n# Standardize the features\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# Create a PCA object with 3 principal components\npca = PCA(n_components=3)\n\n# Fit the PCA model on the standardized data\npca.fit(X_std)\n\n# Transform the data to the new coordinate system\nX_pca = pca.transform(X_std)\n\n# Split the transformed data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n\n# Create an SVM object with a linear kernel\nsvm = SVC(kernel='linear', random_state=42)\n\n# Fit the SVM model on the training data\nsvm.fit(X_train, y_train)\n\n# Predict the target variable for the testing data\ny_pred = svm.predict(X_test)\n\n# Compute the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\n\n# Print the accuracy of the model\nprint(\"Accuracy:\", accuracy)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.775330396475771\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}